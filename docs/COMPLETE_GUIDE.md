# Complete Workshop Guide - From Idea to Working System

## Workshop Overview
**Course**: MIT 6.S093 - How to Ship Almost Anything with AI
**Workshop**: Workshop 1 - Build a Social Media Post Generator
**Date**: January 4, 2026
**Student**: Quilee Simeon (@qsimeon on Mastodon)

## The Journey: From Concept to Working System

### Phase 1: The Idea (Where We Started)

**User's Initial Concept**:
> "My company idea is to use advanced VLM combined with image segmentation pipelines like SAM3D to do inventory tracking for stores. The idea is that an application uses the camera images from security cameras and creates an inventory that has 3D renderings of all the stock in the store."

**What We Did**:
1. Refined the rough idea into a detailed business plan
2. Named the company: **InventoryVision AI**
3. Developed the core value proposition:
   - Uses existing security cameras (no new hardware)
   - VLM (Vision Language Models) for product identification
   - SAM3D for 3D object segmentation and reconstruction
   - Creates "digital twin" of store inventory
   - Automatic daily reconciliation

### Phase 2: Company Documentation

**Goal 1**: Create 3-5 company docs ‚úÖ

We created **5 comprehensive documents**:

1. **Company Overview** (`01_company_overview.md`)
   - Mission statement
   - Problem we solve (retail shrinkage, manual counting)
   - Solution (AI-powered visual inventory)
   - Target market (SMB retailers ‚Üí enterprise)
   - Competitive advantages

2. **Technical Architecture** (`02_technical_architecture.md`)
   - VLM + SAM3D pipeline details
   - Multi-camera fusion engine
   - 3D reconstruction process
   - Edge/cloud processing layers
   - Privacy-first design
   - API ecosystem

3. **Product Features** (`03_product_features.md`)
   - 8 core features (zero-touch tracking, 3D digital twin, etc.)
   - Advanced analytics
   - Integration capabilities (POS, IMS, ERP)
   - User experience features

4. **Business Model** (`04_business_model.md`)
   - SaaS pricing tiers (Starter $299, Pro $799, Enterprise custom)
   - Market analysis (TAM, SAM, SOM)
   - Go-to-market strategy (4 phases)
   - Revenue projections
   - Funding requirements

5. **Brand Voice** (`05_brand_voice.md`)
   - Brand personality (innovative, practical, trustworthy)
   - Content pillars
   - Social media strategy
   - Sample posts for LinkedIn, Twitter, Mastodon
   - Writing guidelines

**How We Generated These**:
- I (Claude) analyzed the user's rough concept
- Expanded it into a complete business model
- Created detailed technical specifications
- Developed brand messaging and voice
- Wrote sample social media posts

### Phase 3: LLM Post Generator

**Goal 2**: Use LLM to generate social media posts ‚úÖ

**Challenges We Faced**:

#### Challenge 1: OpenRouter Credit Issues
**What Happened**:
```
Error code: 402 - This request requires more credits
```

**Why It Failed**:
- OpenRouter account had $0.00 balance
- Even "free tier" models need account credits
- Requested up to 8192 tokens but could only afford 1333

**Attempted Fix #1**: Created new OpenRouter API key
- Result: Same issue (new accounts start with $0)

**Attempted Fix #2**: Used free Gemini model through OpenRouter
```
Error code: 429 - google/gemini-2.0-flash-exp:free is temporarily rate-limited
```
- Free tier models heavily throttled during peak usage
- Not reliable for production use

**Final Solution**: Use OpenAI API directly
- User already had OpenAI API key with credits
- GPT-4o-mini: $0.002-0.005 per post (very cheap)
- Fast, reliable, no rate limits
- ‚úÖ This worked!

#### Challenge 2: Character Limit Issues
**What Happened**:
```
Error: Validation failed: Text character limit of 500 exceeded
Generated post: 1238 characters ‚ùå
```

**Why It Failed**:
- Mastodon has strict 500 character total limit
- Initial prompts didn't enforce this
- LLM generated long-form content (1200+ chars)

**How We Fixed It**:
1. Added character limit to Pydantic schema
2. Updated system prompts with multiple warnings
3. Set strict limit: 350 characters for content
4. Reduced hashtags from 5 to 3-4 short ones
5. Disabled call-to-action for Mastodon

**Result**:
```
Generated post: 352 characters
With hashtags: 402 characters ‚úÖ
Successfully posted!
```

**Implementation Details**:

```python
class SocialMediaPost(BaseModel):
    content: str = Field(
        description="CRITICAL: For Mastodon, must be MAXIMUM 350 characters"
    )
    hashtags: list[str] = Field(
        description="3-4 relevant hashtags. Keep tags short."
    )
    call_to_action: str | None = Field(
        description="For Mastodon, skip this to save characters."
    )
```

**Key Technologies**:
- OpenAI GPT-4o-mini for generation
- Pydantic for structured outputs (type-safe JSON)
- Character counting and validation
- Platform-specific guidelines

### Phase 4: Mastodon Integration

**Goal 3**: Post to Mastodon ‚úÖ

**Setup Process**:
1. User created Mastodon account: @qsimeon@mastodon.social
2. Created application in Mastodon settings
3. Generated access token with read/write permissions
4. Added token to `.env` file

**Implementation**:
- Wrapper class for Mastodon API
- Post status updates
- Search for posts
- Reply to posts
- Error handling and validation

**Successful Posts**:
- Post 1: https://mastodon.social/@qsimeon/115838250963515890
- Post 2: https://mastodon.social/@qsimeon/115838259129156300

### Phase 5: Reply Generation

**Goal 4**: Generate replies with structured outputs ‚úÖ

**How It Works**:
1. **Search**: Find posts by keyword (e.g., "retail technology")
2. **Batch Analysis**: Send all 5 posts to LLM at once
3. **Structured Output**: LLM returns JSON with replies for each post
4. **Relevance Scoring**: AI scores each post 1-10
5. **Decision Making**: AI decides if reply adds value
6. **Filtering**: Only show posts meeting threshold (e.g., 6+/10)

**Example Output**:
```
Found 5 posts matching 'retail technology'

Analyzing 5 posts...
‚úì Generated 5 potential replies
‚úì 1 meet relevance threshold (5/10)
‚úì 1 recommended to post

Post ID: 115838250963515890 | Relevance: 10/10 | ‚úì REPLY
Reasoning: This post directly discusses AI-driven inventory
tracking, which aligns perfectly with our company's mission.
Reply: "Absolutely! Embracing AI-driven inventory tracking..."
```

**Pydantic Schema**:
```python
class Reply(BaseModel):
    post_id: str
    reply_content: str
    should_reply: bool  # AI decides
    reasoning: str      # AI explains why
    relevance_score: int  # 1-10 score

class BatchReplies(BaseModel):
    replies: List[Reply]  # All 5 at once
```

### Phase 6: Cleanup and CLI

**What We Did**:
1. Removed unused alternative implementations
2. Created simple `post` CLI command
3. Updated main.py for full workflow
4. Added comprehensive documentation

## How to Use This System

### Quick Commands

#### 1. Generate and Post (Simplest)
```bash
# Generate a thought leadership post
uv run python post thought_leadership

# Other post types
uv run python post industry_insight
uv run python post product_update
uv run python post customer_story
```

**What happens**:
- Loads your 5 company docs
- Generates post using GPT-4o-mini
- Shows you the post and character count
- Asks if you want to post it
- Posts to Mastodon if you say yes

#### 2. Full Workflow (Post + Replies)
```bash
uv run python src/main.py
```

**What happens**:
1. Generates an original post
2. Posts it to Mastodon (with confirmation)
3. Searches for relevant posts
4. Generates AI replies to relevant posts
5. Shows you reply plan
6. Posts selected replies

#### 3. Test Individual Components
```bash
# Test post generation only
uv run python src/post_generator.py

# Test Mastodon connection
uv run python src/mastodon_client.py

# Test reply generation only
uv run python src/reply_generator.py
```

### Understanding the Code

#### File Structure
```
sundai_01042025/
‚îú‚îÄ‚îÄ post                          # Simple CLI (run this!)
‚îú‚îÄ‚îÄ .env                          # API keys (NEVER commit)
‚îú‚îÄ‚îÄ .gitignore                    # Protects secrets
‚îú‚îÄ‚îÄ company_docs/                 # Your 5 company documents
‚îÇ   ‚îú‚îÄ‚îÄ 01_company_overview.md
‚îÇ   ‚îú‚îÄ‚îÄ 02_technical_architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ 03_product_features.md
‚îÇ   ‚îú‚îÄ‚îÄ 04_business_model.md
‚îÇ   ‚îî‚îÄ‚îÄ 05_brand_voice.md
‚îî‚îÄ‚îÄ src/                          # Source code
    ‚îú‚îÄ‚îÄ post_generator.py         # Generate posts
    ‚îú‚îÄ‚îÄ reply_generator.py        # Generate replies
    ‚îú‚îÄ‚îÄ mastodon_client.py        # Mastodon API
    ‚îî‚îÄ‚îÄ main.py                   # Full workflow
```

#### How `post_generator.py` Works

```python
# 1. Load company docs
docs = load_company_docs()
# Reads all .md files from company_docs/

# 2. Create OpenAI client
client = create_openai_client()
# Uses OPENAI_API_KEY from .env

# 3. Generate post with structured output
post = generate_post(
    company_docs=docs,
    post_type="thought_leadership",  # or industry_insight, etc.
    platform="mastodon",              # enforces char limits
    model="gpt-4o-mini"               # cheap & fast
)

# 4. Format for posting
formatted = format_post_for_platform(post)
# Adds hashtags, formats nicely

# 5. Post to Mastodon
mastodon.post(formatted)
```

#### How Structured Outputs Work

**Before (Traditional LLM)**:
```python
response = client.chat.completions.create(...)
text = response.choices[0].message.content
# Returns: "Here's a post: The future of retail..."
# Problem: Unpredictable format, needs parsing
```

**After (Structured Outputs)**:
```python
class SocialMediaPost(BaseModel):
    content: str
    hashtags: list[str]
    platform: str

response = client.beta.chat.completions.parse(
    response_format=SocialMediaPost
)
post = response.choices[0].message.parsed
# Returns: Guaranteed JSON matching schema
# post.content, post.hashtags are typed!
```

**Benefits**:
- Type-safe (no parsing errors)
- Guaranteed structure
- Easy to work with
- Validation built-in

### API Keys Setup

Your `.env` file contains:
```bash
# OpenAI (what we use)
OPENAI_API_KEY=sk-proj-4X6N...

# Mastodon
MASTODON_ACCESS_TOKEN=rknqinM6...
MASTODON_API_BASE_URL=https://mastodon.social

# Alternative APIs (not currently used)
OPENROUTER_API_KEY=sk-or-v1-0d78...  # Needs credits
ANTHROPIC_API_KEY=sk-ant-api03...     # Could use this
GEMINI_API_KEY=AIzaSyDs...            # Could use this
```

**Security**:
- `.env` is in `.gitignore` (never committed)
- Keys stored as environment variables
- Loaded with `python-dotenv`

### Cost Analysis

Using OpenAI GPT-4o-mini:

| Action | Cost | Tokens |
|--------|------|--------|
| Generate 1 post | $0.002-0.005 | ~1,000 input + 200 output |
| Generate 5 replies | $0.003-0.008 | ~2,000 input + 500 output |
| Full workshop | < $0.10 | All testing + generation |
| Daily posting (1/day) | ~$0.15/month | Very affordable |
| 100 posts | ~$0.50 | Extremely cheap |

**Comparison**:
- OpenRouter free tier: $0 but unreliable (rate limits)
- OpenRouter paid: $0.01-0.03 per post (more expensive)
- OpenAI direct: $0.002-0.005 per post ‚úÖ (cheapest & reliable)

## OpenRouter: What Happened?

### We Never Got OpenRouter Working

**Attempts Made**:
1. ‚úó Used existing API key ‚Üí 402 error (no credits)
2. ‚úó Created new API key ‚Üí same issue (starts with $0)
3. ‚úó Tried free Gemini model ‚Üí 429 rate limit error
4. ‚úì **Switched to OpenAI direct** ‚Üí worked perfectly

### Why OpenRouter Failed

1. **Credit Requirement**
   - All models (even free ones) need account balance
   - Account balance: $0.00
   - Minimum viable: ~$5

2. **Rate Limiting**
   - Free tier models heavily throttled
   - Shared capacity across all users
   - Unreliable during peak hours

3. **Not Worth It For This Use Case**
   - OpenAI direct is cheaper ($0.002 vs $0.01+)
   - More reliable (no throttling)
   - Faster (no proxy overhead)

### When to Use OpenRouter

**Good use cases**:
- Need to compare multiple models
- Want access to models not on OpenAI (e.g., Llama, Mistral)
- Building model routing logic
- Need unified API for many providers

**Not good for**:
- Production apps needing reliability
- Cost-sensitive applications (often more expensive)
- Free tier (too unreliable)

### How to Use OpenRouter (If You Add Credits)

1. **Add credits**: https://openrouter.ai/settings/credits ($5-10)

2. **Update code**:
```python
client = OpenAI(
    api_key=os.getenv('OPENROUTER_API_KEY'),
    base_url="https://openrouter.ai/api/v1"
)

response = client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet",  # or any model
    messages=[...]
)
```

3. **Models available**:
   - `anthropic/claude-3.5-sonnet` (~$0.015/post)
   - `google/gemini-pro` (~$0.001/post)
   - `meta-llama/llama-3-70b` (~$0.008/post)
   - `openai/gpt-4o-mini` (~$0.003/post)

## Workshop Goals: Final Status

### ‚úÖ Goal 1: Create 3-5 Company Docs
**Status**: Complete - 5 comprehensive documents

**What we created**:
- Company overview with mission and market
- Technical architecture with VLM + SAM3D details
- Product features and benefits
- Business model with pricing and GTM
- Brand voice and social media guidelines

**Process**: Started with rough idea ‚Üí AI refined into detailed docs

---

### ‚úÖ Goal 2: LLM-Generated Posts
**Status**: Complete - Structured outputs with char limits

**Technologies used**:
- OpenAI GPT-4o-mini
- Pydantic for structured outputs
- Platform-specific prompts

**Challenges overcome**:
- Character limit enforcement (500 char Mastodon limit)
- Multiple prompt iterations for compliance

**Example output**:
```
In 2023, retailers faced a staggering $99 billion in inventory
shrinkage. The solution? Embrace technology! AI-driven inventory
tracking provides real-time insights...

#RetailTech #InventoryLoss #AIinRetail
```

---

### ‚úÖ Goal 3: Mastodon Integration
**Status**: Complete - Successfully posting

**Setup steps**:
1. Created Mastodon account (@qsimeon)
2. Generated application access token
3. Implemented API wrapper
4. Successfully posted multiple times

**Successful posts**:
- https://mastodon.social/@qsimeon/115838250963515890
- https://mastodon.social/@qsimeon/115838259129156300

---

### ‚úÖ Goal 4: Reply Generation with Structured Outputs
**Status**: Complete - Batch processing working

**How it works**:
1. Search for "retail technology" posts
2. Get 5 most recent results
3. Send all 5 to LLM in single request
4. LLM analyzes all at once with structured output
5. Returns: relevance score, should_reply decision, reasoning, reply text
6. Filter and present recommendations

**Example result**:
```
Found 5 posts
‚úì Generated 5 potential replies
‚úì 1 meet relevance threshold (5/10)
‚úì 1 recommended to post

Post ID: 115838250963515890 | Relevance: 10/10 | ‚úì REPLY
Reasoning: Directly discusses AI-driven inventory tracking
Reply: "Absolutely! Embracing AI-driven inventory tracking..."
```

---

## Key Learnings

### 1. Direct API > API Aggregators (For This Use Case)
- OpenRouter adds complexity without benefits
- Direct APIs are cheaper and more reliable
- Only use aggregators when you need model diversity

### 2. Structured Outputs Are Powerful
- Type-safe, predictable responses
- No parsing errors
- Easy to work with
- Built-in validation

### 3. Character Limits Need Strong Enforcement
- LLMs ignore soft limits
- Need multiple explicit warnings
- Set conservative limits (350 for 500 total)
- Validate before posting

### 4. Start with Simple CLI
- Complex workflows can be intimidating
- Simple `post` command is much more usable
- Progressive disclosure (simple ‚Üí advanced)

### 5. Documentation Matters
- Tell the whole story (including failures)
- Explain why decisions were made
- Simple examples first
- Troubleshooting section essential

## Troubleshooting

### "OPENAI_API_KEY not found"
```bash
# Check .env file
cat .env | grep OPENAI_API_KEY

# Should show your key
# If missing, add it:
echo "OPENAI_API_KEY=sk-proj-..." >> .env
```

### "Character limit exceeded"
- This should be fixed now
- If it happens: post is too long
- Try generating again (it varies)
- Or manually edit the post

### "Rate limit" errors
- OpenAI: 500 req/min (plenty for this)
- Mastodon: 300 posts/hour (more than enough)
- If hit: wait a few minutes, try again

### OpenRouter "402" or "429" errors
- 402: Need to add credits
- 429: Rate limited (free tier)
- **Solution**: Use OpenAI direct instead

## Next Steps

### Immediate
1. ‚úÖ Test all 4 goals (done!)
2. ‚úÖ Generate and post content (done!)
3. Try reply generation: `uv run python src/reply_generator.py`

### Short Term
- Generate posts daily
- Build engagement on Mastodon
- Experiment with different post types
- Track what resonates

### Optional Enhancements
- **Scheduling**: Use cron for daily posts
- **Analytics**: Track engagement metrics
- **A/B Testing**: Test post formats
- **Multi-platform**: Add Twitter, LinkedIn
- **Images**: Generate images with DALL-E
- **Monitoring**: Alert on low engagement

## Summary

**What we built**:
- Complete AI-powered social media automation system
- From rough concept to working code
- 5 comprehensive company documents
- LLM post generator with structured outputs
- Mastodon integration with posting and replies
- Batch reply analysis with relevance scoring

**Time invested**: ~2-3 hours including troubleshooting

**Lines of code**: ~500 lines of clean, well-documented Python

**Cost to run**: ~$0.003 per post (~0.3 cents)

**Workshop status**: ‚úÖ ALL 4 GOALS COMPLETE

**Result**: Production-ready social media automation! üéâ

---

## Appendix: All Commands Reference

```bash
# Quick posting (recommended)
uv run python post thought_leadership
uv run python post industry_insight
uv run python post product_update
uv run python post customer_story

# Full workflow
uv run python src/main.py

# Test components
uv run python src/post_generator.py
uv run python src/mastodon_client.py
uv run python src/reply_generator.py

# Alternative (direct execution)
./post thought_leadership
```

## Contact & Resources

- **Workshop**: MIT 6.S093 - How to Ship Almost Anything with AI
- **Website**: https://iap.sundai.club
- **Mastodon**: @qsimeon@mastodon.social
- **Repository**: /Users/quileesimeon/sundai_01042025

**Congratulations on completing Workshop 1!** üéì
